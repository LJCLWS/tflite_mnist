# 007_mnist_train

## 1) TensorFlowで学習
トレーニングのさせ方として

- 通常のfloatでのトレーニングと
- 量子化のためのQuantization-awareトレーニング

を行う。


#### 実行方法
```
 > ./run_train.sh     　# float トレーニング
 > ./run_train.sh -q  　# Quantization-aware トレーニング
```


#### トレーニング結果の Accuracy

|                      |Float training|Quantization-aware training|
|:--------------------:|:------------:|:-------------------------:|
|TensorFlow 1.12       | 0.9922       | 0.9917                    |
|TensorFlow 1.13       | 0.9923       | 0.9925                    |
|TensorFlow 1.14       | 0.9925       | 0.9920                    |

TensorFlowのバージョンが違っても、トレーニング結果に殆ど差は出ない。


## 2) tflite形式へ変換
```
 > ./run_export.sh    　           # 推論用グラフ生成
 > ./run_convert_to_tflite.sh    　# tflite 形式へ変換
```

## 3) TensorFlow Lite で推論実行
- RaspberryPi 3B の TensorflowLiteで推論実行した時の accuracy と処理時間を計測。
- TensorFlow Lite のバージョン違いによる推論性能の違いも見てみる。

```
 > cd 100_mnist_infer_tflite-cpp
 > ./mnist_infer ../007_mnist_train/mnist_frozengraph_float.tflite
 > ./mnist_infer ../007_mnist_train/mnist_frozengraph_quant.tflite
```


### TensowFlow 1.12 の学習結果で推論実行
- TensowFlow 1.12 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train        |
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------:|
| 1.12       | 1.12       | 1.12           | 0.9922 <br> 31.39[ms] | 0.9917 <br>  10.85 [ms]  |
| 1.12       | 1.12       | 1.13           | 0.9922 <br> 32.41[ms] | 0.9917 <br>  10.83 [ms]  |
| 1.12       | 1.12       | 1.14           | 0.9922 <br> 61.61[ms] | 0.9917 <br> **6.190[ms]**|

-  TensowFlow 1.13 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train        |
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------:|
| 1.12       | 1.13       | 1.12           | 0.9922 <br> 30.31[ms] | 0.9917 <br>  11.94 [ms]  |
| 1.12       | 1.13       | 1.13           | 0.9922 <br> 33.38[ms] | 0.9917 <br>  12.11 [ms]  |
| 1.12       | 1.13       | 1.14           | 0.9922 <br> 62.00[ms] | 0.9917 <br> **6.790[ms]**|

- TensowFlow 1.14 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train        |
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------:|
| 1.12       | 1.14       | 1.12           | 0.9922 <br> 28.54[ms] | 0.9917 <br>  12.10 [ms]  |
| 1.12       | 1.14       | 1.13           | 0.9922 <br> 30.55[ms] | 0.9917 <br>  12.20 [ms]  |
| 1.12       | 1.14       | 1.14           | 0.9922 <br> 58.34[ms] | 0.9917 <br> **6.854[ms]**|


### TensowFlow 1.13 の学習結果で推論実行
- TensowFlow 1.12 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train        |
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------:|
| 1.13       | 1.12       | 1.12           | 0.9923 <br> 30.95[ms] | 0.9924 <br>  11.46 [ms]  |
| 1.13       | 1.12       | 1.13           | 0.9923 <br> 32.72[ms] | 0.9924 <br>  12.10 [ms]  |
| 1.13       | 1.12       | 1.14           | 0.9923 <br> 62.14[ms] | 0.9924 <br> **7.134[ms]**|

- TensowFlow 1.13 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train        |
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------:|
| 1.13       | 1.13       | 1.12           | 0.9923 <br> 30.31[ms] | 0.9924 <br>  11.21 [ms]  |
| 1.13       | 1.13       | 1.13           | 0.9923 <br> 33.95[ms] | 0.9924 <br>  11.23 [ms]  |
| 1.13       | 1.13       | 1.14           | 0.9923 <br> 61.78[ms] | 0.9924 <br> **6.361[ms]**|

- TensowFlow 1.14 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train        |
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------:|
| 1.13       | 1.14       | 1.12           | 0.9923 <br> 28.01[ms] | 0.9924 <br>  11.57 [ms]  |
| 1.13       | 1.14       | 1.13           | 0.9923 <br> 29.96[ms] | 0.9924 <br>  12.06 [ms]  |
| 1.13       | 1.14       | 1.14           | 0.9923 <br> 58.35[ms] | 0.9924 <br> **6.643[ms]**|


### TensowFlow 1.14 の学習結果で推論実行
- TensowFlow 1.12 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train        |
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------:|
| 1.14       | 1.12       | 1.12           | エラー※1             | エラー※1                |
| 1.14       | 1.12       | 1.13           | エラー※1             | エラー※1                |
| 1.14       | 1.12       | 1.14           | エラー※1             | エラー※1                |

- TensowFlow 1.13 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train        |
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------:|
| 1.14       | 1.13       | 1.12           | エラー※1             | エラー※1                |
| 1.14       | 1.13       | 1.13           | エラー※1             | エラー※1                |
| 1.14       | 1.13       | 1.14           | エラー※1             | エラー※1                |

- TensowFlow 1.14 のコンバータで変換
|学習<br>(TF)|変換<br>(TF)|推論<br>(TFLite)| Float train           | Quant-aware train         |Post-train Quant<br>(w/carib)|Post-train Quant<br>(wo/carib)|
|:----------:|:----------:|:--------------:| :-------------------: | :-----------------------: | :--------------------------:| :---------------------------:|
| 1.14       | 1.14       | 1.12           | 0.9925 <br> 27.52[ms] | 0.9921 <br>  10.91 [ms]   | エラー※2                   | エラー※2                    |
| 1.14       | 1.14       | 1.13           | 0.9925 <br> 29.61[ms] | 0.9921 <br>  11.14 [ms]   | エラー※2                   | エラー※2                    |
| 1.14       | 1.14       | 1.14           | 0.9925 <br> 57.56[ms] | 0.9921 <br> **6.234[ms]** | 0.9921 <br> **6.304[ms]**   | 0.9926 <br> 16.585[ms]       |


- ※1) コンバートエラー (GraphDef Versionが異なる)
- ※2) Didn't find op for builtin opcode 'CONV_2D' version '2'


#### 備考
- TF1.14でquantの推論実行速度が大幅向上 (逆に float が悪化してるのは謎）
- post-training量子化において、llキャリブレーション有無で処理速度がかなり異なる。<br>
  ウェイトは両社とも量子化済みだが、実際の演算をintでするかfloatに戻してから演算するかの違いか？

